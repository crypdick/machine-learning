\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\author{Richard Decal}
\title{%
  Solving Cartpole Using Reinforcement Learning and Policy Gradients \\
  \large Machine Learning Engineer Nanodegree\\
  Capstone Proposal}
\begin{document}

\maketitle


%## Proposal
%_(approx. 2-3 pages)_

\section*{Domain Background}
%_(approx. 1-2 paragraphs)_
%In this section, provide brief details on the background information of the domain from which the project is proposed. Historical information relevant to the project should be included. It should be clear how or why a problem in the domain can or should be solved. Related academic research should be appropriately cited in this section, including why that research is relevant. Additionally, a discussion of your personal motivation for investigating a particular problem in the domain is encouraged but not required.
%
%
Naive learner take an environment and set of possible actions to complete an unknown goal. As it acts, it recieves feedback in the form of rewards. Goal is to get the learner to interact with the environment and converge to a decision policy which maximizes rewards.

%
%
%
\section*{Problem Statement}
%_(approx. 1 paragraph)_
%
%In this section, clearly describe the problem that is to be solved. The problem described should be well defined and should have at least one relevant potential solution. Additionally, describe the problem thoroughly such that it is clear that the problem is quantifiable (the problem can be expressed in mathematical or logical terms) , measurable (the problem can be measured by some metric and clearly observed), and replicable (the problem can be reproduced and occurs more than once).
%
The problem I want to solve is finding an optimal policy for learning to play "CartPole" TODO, a simple game which simulates an upright pole hinged on a moveable cart. The game is initated with the cart at the center and the pole upright. The game ends when the pole tilts 15$\deg$ away from vertical, or when the cart has translated to the edge of the screen. The game is considered solved when it stays a long time TODO 

there are two possible actions, left and right. The state space is comprised of the x position, x velocity, the angle theta, and the rate of change of theta. These are all continuous values in the follow bounds: TODO. The rewards are 

\section*{Datasets and Inputs}
%_(approx. 2-3 paragraphs)_
%
%In this section, the dataset(s) and/or input(s) being considered for the project should be thoroughly described, such as how they relate to the problem and why they should be used. Information such as how the dataset or input is (was) obtained, and the characteristics of the dataset or input, should be included with relevant references and citations as necessary It should be clear how the dataset(s) or input(s) will be used in the project and whether their use is appropriate given the context of the problem.
%
The only inputs to the learner is the state, and reward. The Q-table is populated with time.

\section*{Solution Statement}
%_(approx. 1 paragraph)_
%
%In this section, clearly describe a solution to the problem. The solution should be applicable to the project domain and appropriate for the dataset(s) or input(s) given. Additionally, describe the solution thoroughly such that it is clear that the solution is quantifiable (the solution can be expressed in mathematical or logical terms) , measurable (the solution can be measured by some metric and clearly observed), and replicable (the solution can be reproduced and occurs more than once).
%
I predict the solution is to rapidly alternate between moving the cart left and right. This should keep the pole balanced upright. Inevitably, the platform is translated away from the center, which puts the trial at risk of ending prematurely for moving out-of-bounds. In that situation, an ideal policy would be to bias the cart to move towards the center.
%
\section*{Benchmark Model}
%_(approximately 1-2 paragraphs)_
%
%In this section, provide the details for a benchmark model or result that relates to the domain, problem statement, and intended solution. Ideally, the benchmark model or result contextualizes existing methods or known information in the domain and problem given, which could then be objectively compared to the solution. Describe how the benchmark model or result is measurable (can be measured by some metric and clearly observed) with thorough detail.
%
Benchmark is random agent. If negative angle, go left, else go right. A second benchmark is always push the cart in the direction the pole is leaning: if theta is negative, move left so that the pole tilts to the right (and vice versa).
%
\section*{Evaluation Metrics}
%_(approx. 1-2 paragraphs)_
%
%In this section, propose at least one evaluation metric that can be used to quantify the performance of both the benchmark model and the solution model. The evaluation metric(s) you propose should be appropriate given the context of the data, the problem statement, and the intended solution. Describe how the evaluation metric(s) are derived and provide an example of their mathematical representations (if applicable). Complex evaluation metrics should be clearly defined and quantifiable (can be expressed in mathematical or logical terms).
%
%
%
\section*{Project Design}
%_(approx. 1 page)_
%
%In this final section, summarize a theoretical workflow for approaching a solution given the problem. Provide thorough discussion for what strategies you may consider employing, what analysis of the data might be required before being used, or which algorithms will be considered for your implementation. The workflow and discussion that you provide should align with the qualities of the previous sections. Additionally, you are encouraged to include small visualizations, pseudocode, or diagrams to aid in describing the project design, but it is not required. The discussion should clearly outline your intended workflow of the capstone project.



Q-learning is semi exhaustive. Try all the state,action pairs, and store it in a Q-table. Later, when it encounters the state it looks up which action is expected to have the largest reward. 

Policy gradients are better because (there are mathematical guarantees that it is always gettting better). to reduce variance but keep low bias:
compute the gradient of a single reward at a specific timestep. we don't assume we need to know anything about reward-f(x),


I propose to use Tensorflow. This will help me visualize computational graph of the model.

%-----------
%
%**Before submitting your proposal, ask yourself. . .**
%
%- Does the proposal you have written follow a well-organized structure similar to that of the project template?
%- Is each section (particularly **Solution Statement** and **Project Design**) written in a clear, concise and specific fashion? Are there any ambiguous terms or phrases that need clarification?
%- Would the intended audience of your project be able to understand your proposal?
%- Have you properly proofread your proposal to assure there are minimal grammatical and spelling mistakes?
%- Are all the resources used for this project correctly cited and referenced?

\end{document}